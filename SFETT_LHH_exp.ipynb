{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "import wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sfett.tensors import TTSFTuckerTensor\n",
    "import torch.backends.opt_einsum as opt_einsum\n",
    "from tqdm.notebook import tqdm\n",
    "from sfett.tools import (\n",
    "    generate_laplace_TT,\n",
    "    tt_scalar_prod,\n",
    "    henon_heiles,\n",
    ")\n",
    "from IPython.display import clear_output\n",
    "from sfett.manifolds import TTSFTuckerManifold\n",
    "from sfett.tensors import tt_sf_tucker_scalar_prod, TTSFTuckerTangentVector\n",
    "import random\n",
    "\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "opt_einsum.strategy = \"greedy\"\n",
    "import torch\n",
    "import tntorch as tn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda:0\"\n",
    "import scipy\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger_name = str(datetime.now())\n",
    "print(logger_name)\n",
    "logging.basicConfig(filename=f\".logs/Hilbert_exp_{logger_name}.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_quad(\n",
    "    X: TTSFTuckerTensor,\n",
    "    Y: Optional[TTSFTuckerTensor],\n",
    "    laplace_mat: List[torch.Tensor],\n",
    "    potenital: Optional[TTSFTuckerTensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    N = len(laplace_mat)\n",
    "    d_s = X.shared_factors_amount\n",
    "    d_t = N - d_s\n",
    "    new_laplace_TT = []\n",
    "    for i in range(N):\n",
    "        a_factor = X.tucker_factors[i if i < d_t else -1]\n",
    "        b_factor = a_factor if Y is None else Y.tucker_factors[i if i < d_t else -1]\n",
    "        laplace_core = laplace_mat[i]\n",
    "\n",
    "        new_laplace_TT.append(\n",
    "            torch.einsum(\"abcd,br,cy->aryd\", laplace_core, a_factor, b_factor)\n",
    "        )\n",
    "    res = tt_scalar_prod(\n",
    "        new_laplace_TT, X.tt_cores, None if Y is None else Y.tt_cores\n",
    "    ) + (\n",
    "        0\n",
    "        if potenital is None\n",
    "        else tt_sf_tucker_scalar_prod(\n",
    "            X, potenital.hadamard_product(X if Y is None else Y)\n",
    "        )\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def steepest_desc(\n",
    "    X: TTSFTuckerTensor,\n",
    "    direction: TTSFTuckerTensor,\n",
    "    bilinear_A: Callable[[TTSFTuckerTensor, TTSFTuckerTensor], torch.Tensor],\n",
    ") -> float:\n",
    "    _a, _c, _b = (\n",
    "        bilinear_A(X, X),  # x.T A x\n",
    "        bilinear_A(direction, direction),  # grad.T A grad\n",
    "        bilinear_A(X, direction),  # x.T A grad\n",
    "    )\n",
    "    STAS = torch.Tensor([[_a, _b], [_b, _c]], device=\"cpu\")\n",
    "    _a = tt_sf_tucker_scalar_prod(X, X)\n",
    "    _c = tt_sf_tucker_scalar_prod(direction, direction)\n",
    "    _b = tt_sf_tucker_scalar_prod(X, direction)\n",
    "    STBS = torch.Tensor([[_a, _b], [_b, _c]], device=\"cpu\")\n",
    "    _, eig_vectors = scipy.linalg.eigh(\n",
    "        STAS,\n",
    "        STBS,\n",
    "    )\n",
    "    alpha, beta = eig_vectors[0][0], eig_vectors[1][0]\n",
    "    new_X = alpha * X + beta * direction\n",
    "    delta = bilinear_A(X, X) - bilinear_A(new_X, new_X)\n",
    "    assert delta >= 0 or abs(delta) <= 1e-8, f\"{delta = } the nex step is not optimal!\"\n",
    "    return new_X\n",
    "\n",
    "\n",
    "def grad_desc(\n",
    "    bilinear_A: Callable[[TTSFTuckerTensor, TTSFTuckerTensor], torch.Tensor],\n",
    "    X: TTSFTuckerTensor,\n",
    "    lb_opt: float,\n",
    "    max_iters: int = 100,\n",
    "    lr: Union[float, Callable[[int], float]] = 1e-4 / 5,\n",
    "    eps: float = 1e-6,\n",
    "):\n",
    "    X = X / X.norm()\n",
    "    tt_ranks = X.tt_ranks\n",
    "    sf_tucker_ranks = X.tucker_ranks\n",
    "    obj_fun_grad = TTSFTuckerManifold().grad(partial(bilinear_A, Y=None))\n",
    "    fun_log = [bilinear_A(X, X).cpu()]\n",
    "    grad_log = []\n",
    "    rel_errs = [torch.abs(fun_log[-1] - lb_opt) / lb_opt]\n",
    "    iters = tqdm(range(max_iters))\n",
    "    for _ in iters:\n",
    "        h = obj_fun_grad(X)\n",
    "        h = h.construct() - 2 * fun_log[-1] * X\n",
    "        grad_log.append(h.norm().cpu())\n",
    "        if isinstance(lr, float):\n",
    "            X = X - (lr * h)\n",
    "        else:\n",
    "            try:\n",
    "                X = lr(X, h, bilinear_A)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return fun_log, grad_log, rel_errs\n",
    "\n",
    "        X.round(tt_ranks=tt_ranks, sf_tucker_ranks=sf_tucker_ranks)\n",
    "        X = (1 / X.norm()) * X\n",
    "        fun_log.append(bilinear_A(X, X).cpu())\n",
    "        rel_errs.append(torch.abs(fun_log[-1] - lb_opt) / lb_opt)\n",
    "        iters.set_description(\n",
    "            f\"{fun_log[-1]:.2f}\" + f\" {grad_log[-1]:.2f}\" + f\" {rel_errs[-1]:.2f}\"\n",
    "        )\n",
    "        if (rel_errs[-1]) < eps:\n",
    "            return fun_log, grad_log, rel_errs\n",
    "    return fun_log, grad_log, rel_errs\n",
    "\n",
    "\n",
    "def rr_scipy(\n",
    "    S: List[TTSFTuckerTensor],\n",
    "    bilinear_A: Callable[[TTSFTuckerTensor, TTSFTuckerTensor], torch.Tensor],\n",
    "):\n",
    "    vecs_am = len(S)\n",
    "    device = S[0].device\n",
    "    p = S[0].batch_size\n",
    "    STBS = torch.zeros((vecs_am, vecs_am, p, p), device=device)\n",
    "    STAS = torch.zeros((vecs_am, vecs_am, p, p), device=device)\n",
    "    for i in range(vecs_am):\n",
    "        for j in range(i, vecs_am):\n",
    "            STBS[i, j] = S[i] @ S[j]\n",
    "            STAS[i, j] = bilinear_A(S[i].clone(), S[j].clone())\n",
    "            if i != j:\n",
    "                STBS[j, i] = STBS[i, j].T\n",
    "                STAS[j, i] = STAS[i, j].T\n",
    "    STBS = STBS.transpose(1, 2).reshape(vecs_am * p, vecs_am * p)\n",
    "    STAS = STAS.transpose(1, 2).reshape(vecs_am * p, vecs_am * p)\n",
    "    try:\n",
    "        eig_vals, eig_vectors = scipy.linalg.eigh(STAS.to(\"cpu\"), STBS.to(\"cpu\"))\n",
    "    except Exception as e:\n",
    "        print(torch.linalg.eigh(STAS).eigenvalues, torch.linalg.eigh(STBS).eigenvalues)\n",
    "        raise e\n",
    "    eig_vecs = torch.Tensor(eig_vectors[:, :p]).to(device)\n",
    "    eig_vals = torch.Tensor(eig_vals[:p]).to(device)\n",
    "    if p == 1:\n",
    "        eig_vals = eig_vals.unsqueeze(-1)\n",
    "    return eig_vecs, eig_vals\n",
    "\n",
    "\n",
    "def lobpcg(\n",
    "    bilinear_A: Callable[[TTSFTuckerTensor, TTSFTuckerTensor], torch.Tensor],\n",
    "    X: TTSFTuckerTensor,\n",
    "    rayleigh_ritz: Callable[[List[TTSFTuckerTensor], Callable], List[torch.Tensor]],\n",
    "    lb_opt: float,\n",
    "    max_iters: int = 100,\n",
    "    eps: float = 1e-6,\n",
    "):\n",
    "    rayleigh_ritz = rr_scipy\n",
    "    obj_fun_grad = TTSFTuckerManifold().grad(partial(bilinear_A, Y=None))\n",
    "    X = (1 / X.norm()) * X\n",
    "    _, theta = rayleigh_ritz([X], bilinear_A)\n",
    "    r_tt, r_t = X.tt_ranks, X.tucker_ranks\n",
    "    fun_log = [bilinear_A(X, X).cpu()]\n",
    "    rel_errs = [torch.abs(fun_log[-1] - lb_opt) / lb_opt]\n",
    "    R: TTSFTuckerTensor = 0.5 * obj_fun_grad(X).construct() - theta * X\n",
    "    grad_log = [R.norm().cpu()]\n",
    "    P: Optional[TTSFTuckerTensor] = None\n",
    "    iters = tqdm(range(max_iters))\n",
    "    for _ in iters:\n",
    "        start = time.time()\n",
    "        H = R\n",
    "        S = [X, H] + ([] if P is None else [P])\n",
    "        try:\n",
    "            C, theta = rayleigh_ritz(S, bilinear_A)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return fun_log, grad_log, rel_errs\n",
    "        direction: TTSFTuckerTensor = C[1] * H\n",
    "        if P is not None:\n",
    "            direction += C[2] * P\n",
    "        X = C[0] * X + direction\n",
    "        P = direction\n",
    "\n",
    "        X.round(tt_ranks=r_tt, sf_tucker_ranks=r_t)\n",
    "        X = (1 / X.norm()) * X\n",
    "\n",
    "        R = 0.5 * obj_fun_grad(X).construct() - theta * X\n",
    "        projector = TTSFTuckerManifold().grad(partial(tt_sf_tucker_scalar_prod, B=P))\n",
    "        P = projector(X)\n",
    "        P = P - tt_sf_tucker_scalar_prod(X, P.construct()) * TTSFTuckerTangentVector(X)\n",
    "        P = P.construct()\n",
    "\n",
    "        t = time.time() - start\n",
    "        fun_log.append(bilinear_A(X, X).cpu())\n",
    "        grad_log.append(R.norm().cpu())\n",
    "        rel_errs.append(torch.abs(fun_log[-1] - lb_opt) / lb_opt)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"obj\": fun_log[-1],\n",
    "                \"grad_norm\": grad_log[-1],\n",
    "                \"rel_err\": rel_errs[-1],\n",
    "                \"iter_time\": t,\n",
    "            }\n",
    "        )\n",
    "        iters.set_description(\n",
    "            f\"{fun_log[-1]:.2f}\" + f\" {grad_log[-1]:.2f}\" + f\" {rel_errs[-1]:.2f}\"\n",
    "        )\n",
    "        if (rel_errs[-1]) < eps:\n",
    "            return fun_log, grad_log, rel_errs\n",
    "    return fun_log, grad_log, rel_errs\n",
    "\n",
    "\n",
    "def compute_exp_laplace_no_pot(\n",
    "    laplace_mat: List[torch.Tensor],\n",
    "    max_tt_rank: int,\n",
    "    max_sf_tucker_rank: int,\n",
    "    opt_val: float,\n",
    "    step: int = 2,\n",
    "):\n",
    "    device = laplace_mat[0].device\n",
    "    n = laplace_mat[0].shape[1]\n",
    "    vanilla_laplace_bilinear = partial(laplace_quad, laplace_mat=laplace_mat)\n",
    "    N = len(laplace_mat)\n",
    "    ranks = [(1, 1)]\n",
    "    rank_to_his = {}\n",
    "    rel_err_tol = 1e-6\n",
    "    for rank in tqdm(ranks):\n",
    "        max_tt_rank, max_sf_tucker_rank = rank\n",
    "        tt_cores_shapes = (\n",
    "            [[1, max_sf_tucker_rank, max_tt_rank]]\n",
    "            + [[max_tt_rank, max_sf_tucker_rank, max_tt_rank]] * (N - 2)\n",
    "            + [[max_tt_rank, max_sf_tucker_rank, 1]]\n",
    "        )\n",
    "        tt_cores = [torch.ones(sh, device=device) for sh in tt_cores_shapes]\n",
    "        tucker_factors = [torch.ones(n, max_sf_tucker_rank, device=device)] * N\n",
    "\n",
    "        lobpcg_time_to_ds = {}\n",
    "        lobpcg_rel_err_to_ds = {}\n",
    "        clear_output()\n",
    "        for _d_s in range(1, N + 1, step):\n",
    "            _d_t = N - _d_s\n",
    "            t = TTSFTuckerTensor(\n",
    "                tt_cores=[c.clone() for c in tt_cores],\n",
    "                device=device,\n",
    "                tucker_factors=[f.clone() for f in tucker_factors[:_d_t]]\n",
    "                + [tucker_factors[-1].clone()],\n",
    "                shared_factors_amount=_d_s,\n",
    "            )\n",
    "            t /= t.norm()\n",
    "            t.orthogonalize(-1)\n",
    "\n",
    "            res = 0\n",
    "            wandb.init(\n",
    "                project=...,\n",
    "                name=f\"laplace_no_pot_sh={_d_s}_r={rank}\",\n",
    "                config={\n",
    "                    \"shared_factors\": _d_s,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": \"laplace_no_potenital\",\n",
    "                },\n",
    "            )\n",
    "            start = time.time()\n",
    "            lobpcg_fun_log, _, lobpcg_rel_errs = lobpcg(\n",
    "                vanilla_laplace_bilinear,\n",
    "                t.clone(),\n",
    "                rr_scipy,\n",
    "                lb_opt=opt_val,\n",
    "                max_iters=700,\n",
    "                eps=rel_err_tol,\n",
    "            )\n",
    "            end = time.time()\n",
    "            wandb.finish()\n",
    "            res += end - start\n",
    "            lobpcg_time_to_ds[_d_s] = res\n",
    "            lobpcg_rel_err_to_ds[_d_s] = lobpcg_rel_errs\n",
    "        rank_to_his[rank] = (lobpcg_rel_err_to_ds, lobpcg_time_to_ds)\n",
    "    return rank_to_his\n",
    "\n",
    "\n",
    "def compute_exp_laplace_pot(\n",
    "    laplace_mat: List[torch.Tensor],\n",
    "    pot_mat: List[torch.Tensor],\n",
    "    max_tt_rank: int,\n",
    "    max_sf_tucker_rank: int,\n",
    "    opt_val: float,\n",
    "    step: int = 2,\n",
    "):\n",
    "    device = laplace_mat[0].device\n",
    "    n = laplace_mat[0].shape[1]\n",
    "    vanilla_laplace_bilinear = partial(laplace_quad, laplace_mat=laplace_mat)\n",
    "    N = len(laplace_mat)\n",
    "    ranks = [(1, 1)]\n",
    "    rank_to_his = {}\n",
    "    rel_err_tol = 1e-6\n",
    "    for rank in tqdm(ranks):\n",
    "        print(rank)\n",
    "        max_tt_rank, max_sf_tucker_rank = rank\n",
    "        tt_cores_shapes = (\n",
    "            [[1, max_sf_tucker_rank, max_tt_rank]]\n",
    "            + [[max_tt_rank, max_sf_tucker_rank, max_tt_rank]] * (N - 2)\n",
    "            + [[max_tt_rank, max_sf_tucker_rank, 1]]\n",
    "        )\n",
    "        tt_cores = [torch.ones(sh, device=device) for sh in tt_cores_shapes]\n",
    "        tucker_factors = [torch.ones(n, max_sf_tucker_rank, device=device)] * N\n",
    "\n",
    "        lobpcg_time_to_ds = {}\n",
    "        lobpcg_rel_err_to_ds = {}\n",
    "        clear_output()\n",
    "        for _d_s in range(1, N + 1, step):\n",
    "            _d_t = N - _d_s\n",
    "            t = TTSFTuckerTensor(\n",
    "                tt_cores=[c.clone() for c in tt_cores],\n",
    "                device=device,\n",
    "                tucker_factors=[f.clone() for f in tucker_factors[:_d_t]]\n",
    "                + [tucker_factors[-1].clone()],\n",
    "                shared_factors_amount=_d_s,\n",
    "            )\n",
    "            t /= t.norm()\n",
    "            t.orthogonalize(-1)\n",
    "            henon_heiles_sfett = TTSFTuckerTensor(\n",
    "                shared_factors_amount=_d_s,\n",
    "                tt_cores=[c.clone() for c in pot_mat],\n",
    "                tucker_factors=[torch.eye(n, device=device) for _ in range(_d_t + 1)],\n",
    "            )\n",
    "            henon_heiles_sfett.round()\n",
    "            hh_laplace_bilinear = partial(\n",
    "                laplace_quad,\n",
    "                laplace_mat=[c.clone() for c in laplace_mat],\n",
    "                potenital=henon_heiles_sfett,\n",
    "            )\n",
    "\n",
    "            res = 0\n",
    "            wandb.init(\n",
    "                project=...,\n",
    "                name=f\"laplace_hh_sh={_d_s}_r={rank}\",\n",
    "                config={\"shared_factors\": _d_s, \"rank\": rank, \"name\": \"laplace_hh\"},\n",
    "            )\n",
    "            start = time.time()\n",
    "            lobpcg_fun_log, _, lobpcg_rel_errs = lobpcg(\n",
    "                hh_laplace_bilinear,\n",
    "                t.clone(),\n",
    "                rr_scipy,\n",
    "                lb_opt=opt_val,\n",
    "                max_iters=700,\n",
    "                eps=rel_err_tol,\n",
    "            )\n",
    "            end = time.time()\n",
    "            wandb.finish()\n",
    "            res += end - start\n",
    "            lobpcg_time_to_ds[_d_s] = res\n",
    "            lobpcg_rel_err_to_ds[_d_s] = lobpcg_rel_errs\n",
    "        rank_to_his[rank] = (lobpcg_rel_err_to_ds, lobpcg_time_to_ds)\n",
    "    return rank_to_his\n",
    "\n",
    "\n",
    "def compute_time_laplace_no_pot(\n",
    "    laplace_mat: List[torch.Tensor],\n",
    "    max_tt_rank: int,\n",
    "    max_sf_tucker_rank: int,\n",
    "    opt_val: float,\n",
    "    step=2,\n",
    "):\n",
    "    device = laplace_mat[0].device\n",
    "    n = laplace_mat[0].shape[1]\n",
    "    vanilla_laplace_bilinear = partial(laplace_quad, laplace_mat=laplace_mat)\n",
    "    N = len(laplace_mat)\n",
    "    rank_to_his = {}\n",
    "    rel_err_tol = 1e-6\n",
    "\n",
    "    tt_cores_shapes = (\n",
    "        [[1, max_sf_tucker_rank, max_tt_rank]]\n",
    "        + [[max_tt_rank, max_sf_tucker_rank, max_tt_rank]] * (N - 2)\n",
    "        + [[max_tt_rank, max_sf_tucker_rank, 1]]\n",
    "    )\n",
    "    tt_cores = [torch.randn(sh, device=device) for sh in tt_cores_shapes]\n",
    "    tucker_factors = [torch.randn(n, max_sf_tucker_rank, device=device)] * N\n",
    "\n",
    "    lobpcg_time_to_ds = {}\n",
    "    lobpcg_rel_err_to_ds = {}\n",
    "    for _d_s in range(1, N + 1, step):\n",
    "        _d_t = N - _d_s\n",
    "        t = TTSFTuckerTensor(\n",
    "            tt_cores=[c.clone() for c in tt_cores],\n",
    "            device=device,\n",
    "            tucker_factors=[f.clone() for f in tucker_factors[:_d_t]]\n",
    "            + [tucker_factors[-1].clone()],\n",
    "            shared_factors_amount=_d_s,\n",
    "        )\n",
    "        t /= t.norm()\n",
    "        t.orthogonalize(-1)\n",
    "\n",
    "        wandb.init(\n",
    "            project=...,\n",
    "            name=f\"laplace_no_pot_sh={_d_s}_r={max(max_tt_rank, max_sf_tucker_rank)}_n={n}\",\n",
    "            config={\n",
    "                \"shared_factors\": _d_s,\n",
    "                \"rank\": [max_tt_rank, max_sf_tucker_rank],\n",
    "                \"name\": \"laplace_no_potenital\",\n",
    "                \"n\": n,\n",
    "            },\n",
    "        )\n",
    "        lobpcg_fun_log, _, lobpcg_rel_errs = lobpcg(\n",
    "            vanilla_laplace_bilinear,\n",
    "            t.clone(),\n",
    "            rr_scipy,\n",
    "            lb_opt=opt_val,\n",
    "            max_iters=70000,\n",
    "            eps=rel_err_tol,\n",
    "        )\n",
    "        wandb.finish()\n",
    "        lobpcg_time_to_ds[_d_s] = 0\n",
    "        lobpcg_rel_err_to_ds[_d_s] = lobpcg_rel_errs\n",
    "    rank_to_his[(max_tt_rank, max_sf_tucker_rank)] = (\n",
    "        lobpcg_rel_err_to_ds,\n",
    "        lobpcg_time_to_ds,\n",
    "    )\n",
    "    return rank_to_his\n",
    "\n",
    "\n",
    "def compute_time_laplace_pot(\n",
    "    laplace_mat: List[torch.Tensor],\n",
    "    pot_mat: List[torch.Tensor],\n",
    "    max_tt_rank: int,\n",
    "    max_sf_tucker_rank: int,\n",
    "    opt_val: float,\n",
    "    step: int = 1,\n",
    "):\n",
    "    device = laplace_mat[0].device\n",
    "    n = laplace_mat[0].shape[1]\n",
    "    vanilla_laplace_bilinear = partial(laplace_quad, laplace_mat=laplace_mat)\n",
    "    N = len(laplace_mat)\n",
    "    rank_to_his = {}\n",
    "    rel_err_tol = 1e-6\n",
    "    tt_cores_shapes = (\n",
    "        [[1, max_sf_tucker_rank, max_tt_rank]]\n",
    "        + [[max_tt_rank, max_sf_tucker_rank, max_tt_rank]] * (N - 2)\n",
    "        + [[max_tt_rank, max_sf_tucker_rank, 1]]\n",
    "    )\n",
    "    tt_cores = [torch.randn(sh, device=device) for sh in tt_cores_shapes]\n",
    "    tucker_factors = [torch.randn(n, max_sf_tucker_rank, device=device)] * N\n",
    "\n",
    "    lobpcg_time_to_ds = {}\n",
    "    lobpcg_rel_err_to_ds = {}\n",
    "    for _d_s in range(1, N + 1, step):\n",
    "        _d_t = N - _d_s\n",
    "        t = TTSFTuckerTensor(\n",
    "            tt_cores=[c.clone() for c in tt_cores],\n",
    "            device=device,\n",
    "            tucker_factors=[f.clone() for f in tucker_factors[:_d_t]]\n",
    "            + [tucker_factors[-1].clone()],\n",
    "            shared_factors_amount=_d_s,\n",
    "        )\n",
    "        logger.info(f\"{ t.tt_ranks }, {t.tucker_ranks}\")\n",
    "        logger.info(f\"{ t.params_amount }\")\n",
    "        n = t.tucker_factors[0].shape[0]\n",
    "        t /= t.norm()\n",
    "        t.orthogonalize(-1)\n",
    "        henon_heiles_sfett = TTSFTuckerTensor(\n",
    "            shared_factors_amount=_d_s,\n",
    "            tt_cores=[c.clone() for c in pot_mat],\n",
    "            tucker_factors=[torch.eye(n, device=device) for _ in range(_d_t + 1)],\n",
    "        )\n",
    "        henon_heiles_sfett.round()\n",
    "        hh_laplace_bilinear = partial(\n",
    "            laplace_quad,\n",
    "            laplace_mat=[c.clone() for c in laplace_mat],\n",
    "            potenital=henon_heiles_sfett,\n",
    "        )\n",
    "\n",
    "        wandb.init(\n",
    "            project=...,\n",
    "            name=f\"laplace_hh_sh={_d_s}_r={max(max_tt_rank, max_sf_tucker_rank)}_n={n}\",\n",
    "            config={\n",
    "                \"shared_factors\": _d_s,\n",
    "                \"rank\": [max_tt_rank, max_sf_tucker_rank],\n",
    "                \"name\": \"laplace_hh_potenital\",\n",
    "                \"n\": n,\n",
    "            },\n",
    "        )\n",
    "        lobpcg_fun_log, _, lobpcg_rel_errs = lobpcg(\n",
    "            hh_laplace_bilinear,\n",
    "            t.clone(),\n",
    "            rr_scipy,\n",
    "            lb_opt=opt_val,\n",
    "            max_iters=70000,\n",
    "            eps=rel_err_tol,\n",
    "        )\n",
    "        wandb.finish()\n",
    "        lobpcg_rel_err_to_ds[_d_s] = lobpcg_rel_errs\n",
    "\n",
    "    rank_to_his[(max_tt_rank, max_sf_tucker_rank)] = (\n",
    "        lobpcg_rel_err_to_ds,\n",
    "        lobpcg_time_to_ds,\n",
    "    )\n",
    "    return lobpcg_time_to_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "core_am = 8\n",
    "rank = 2\n",
    "tucker_rank = 2\n",
    "n = 128\n",
    "A = 2\n",
    "domain = [torch.linspace(-A, A, n, device=device)] * core_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_TT = generate_laplace_TT(\n",
    "    n=n, d=core_am, device=device, alpha=(n + 1) ** 2 / ((2 * A) ** 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_heiles_tn = tn.cross(\n",
    "    function=henon_heiles,\n",
    "    domain=domain,\n",
    "    function_arg=\"matrix\",\n",
    "    device=device,\n",
    "    rmax=3,\n",
    ")\n",
    "henon_heiles_TT = henon_heiles_tn.cores\n",
    "print(henon_heiles_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_desc = {\n",
    "    \"Laplace\": r\"${\\langle Lx, x \\rangle}$\",\n",
    "    \"HH\": r\"${\\langle Lx, x \\rangle + \\langle x, V \\odot x \\rangle}$\",\n",
    "}\n",
    "name_to_grad_desc = {\n",
    "    \"Laplace\": r\"$\\|P_{T_x M}\\left(\\nabla_x\\langle Lx, x \\rangle\\right)  - 2\\langle Lx, x \\rangle \\cdot x \\|$\",\n",
    "    \"HH\": r\"$\\|P_{T_x M}\\left(\\nabla_x[\\langle Lx, x \\rangle  + \\langle x, V \\odot x\\rangle]\\right)  - 2\\langle Lx, x \\rangle \\cdot x \\|$\",\n",
    "    \"LOBPCG\": r\"$\\|P_{T_x M}\\left(R\\right)\\|$\",\n",
    "}\n",
    "name_to_opt = {\"Laplace\": 78.8974, \"HH\": 80.10747094}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_his = compute_exp_laplace_no_pot(\n",
    "    laplace_TT,\n",
    "    max_tt_rank=rank,\n",
    "    max_sf_tucker_rank=tucker_rank,\n",
    "    opt_val=name_to_opt[\"Laplace\"],\n",
    "    step=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_his2 = compute_exp_laplace_pot(\n",
    "    laplace_TT,\n",
    "    henon_heiles_TT,\n",
    "    max_tt_rank=rank,\n",
    "    max_sf_tucker_rank=tucker_rank,\n",
    "    opt_val=name_to_opt[\"HH\"],\n",
    "    step=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"=================NEW_EXP================\")\n",
    "logger.info(f\"{datetime.now().timestamp()}\")\n",
    "\n",
    "dicts_laplace_hh_pot = {\n",
    "    100: 80.14891053,\n",
    "    200: 80.15103274,\n",
    "    500: 80.15082747,\n",
    "    700: 80.15066734,\n",
    "    800: 80.15061884,\n",
    "    900: 80.15059055,\n",
    "    1000: 80.15059055,\n",
    "}\n",
    "for n in [900, 100]:\n",
    "\n",
    "    logger.info(n)\n",
    "    r_tt, r_sft = 5, 5\n",
    "    laplace_TT = generate_laplace_TT(n=n, d=core_am, device=device, alpha=(n + 1) ** 2)\n",
    "    domain = [torch.linspace(0, 1, n, device=device)] * core_am\n",
    "    henon_heiles_tn = tn.cross(\n",
    "        function=henon_heiles,\n",
    "        domain=domain,\n",
    "        function_arg=\"matrix\",\n",
    "        device=device,\n",
    "        eps=1e-14,\n",
    "        verbose=False,\n",
    "    )\n",
    "    henon_heiles_TT = henon_heiles_tn.cores\n",
    "    res = compute_time_laplace_pot(\n",
    "        laplace_TT,\n",
    "        henon_heiles_TT,\n",
    "        max_tt_rank=r_tt,\n",
    "        max_sf_tucker_rank=r_sft,\n",
    "        opt_val=dicts_laplace_hh_pot[n],\n",
    "    )\n",
    "    logger.info(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"=================NEW_EXP================\")\n",
    "logger.info(f\"{datetime.now().timestamp()}\")\n",
    "dicts_laplace_no_pot = {\n",
    "    i: 4 * np.sin(np.pi / (2 * (i + 1))) ** 2 * core_am * (i + 1) ** 2\n",
    "    for i in range(100, 1100, 100)\n",
    "}\n",
    "print(dicts_laplace_no_pot)\n",
    "for n in [900, 500, 100]:\n",
    "    logger.info(n)\n",
    "    r_tt, r_sft = 5, 5\n",
    "    laplace_TT = generate_laplace_TT(n=n, d=core_am, device=device, alpha=(n + 1) ** 2)\n",
    "    res = compute_time_laplace_no_pot(\n",
    "        laplace_TT,\n",
    "        max_tt_rank=r_tt,\n",
    "        max_sf_tucker_rank=r_sft,\n",
    "        opt_val=dicts_laplace_no_pot[n],\n",
    "        step=1,\n",
    "    )\n",
    "    logger.info(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
