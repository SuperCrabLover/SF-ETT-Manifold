{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "import wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple, Callable\n",
    "import torch.backends.opt_einsum as opt_einsum\n",
    "import matplotlib.pyplot as plt\n",
    "from sfett.tensors import TTSFTuckerTensor\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "from sfett.manifolds import TTSFTuckerManifold\n",
    "from sfett.tensors import tt_sf_tucker_scalar_prod\n",
    "\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import tntorch as tn\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "from logging import Logger\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "opt_einsum.strategy = \"greedy\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger_name = str(datetime.now())\n",
    "print(logger_name)\n",
    "logging.basicConfig(filename=f\".logs/Hilbert_exp_{logger_name}.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_frob_norm(\n",
    "    target: TTSFTuckerTensor,\n",
    "    X: TTSFTuckerTensor,\n",
    "    max_iters: int = 300,\n",
    ") -> TTSFTuckerTensor:\n",
    "    t_norm = target.norm()\n",
    "\n",
    "    def rel_err(target, _X):\n",
    "        return (target - _X).norm() / t_norm\n",
    "\n",
    "    def objective(_X):\n",
    "        R = target - _X\n",
    "        return tt_sf_tucker_scalar_prod(R, R)\n",
    "\n",
    "    tt_ranks = X.tt_ranks\n",
    "    tucker_ranks = X.tucker_ranks\n",
    "    iters = tqdm(range(max_iters))\n",
    "    fun_log = []\n",
    "    grad_log = []\n",
    "    fun_log.append(rel_err(target, X).cpu())\n",
    "    new_X = X\n",
    "    for _ in iters:\n",
    "        X = new_X\n",
    "        R = target - X\n",
    "        g = -2 * R\n",
    "        projector = TTSFTuckerManifold().grad(partial(tt_sf_tucker_scalar_prod, B=g))\n",
    "        g = projector(X).construct()\n",
    "        g_norm = g.norm()\n",
    "        lr = -1 * tt_sf_tucker_scalar_prod(g / g_norm, R / g_norm)\n",
    "        grad_log.append(g_norm.cpu())\n",
    "        new_X = X - lr * g\n",
    "        new_X.round(tt_ranks=tt_ranks, sf_tucker_ranks=tucker_ranks)\n",
    "        new_err = rel_err(target, new_X).cpu()\n",
    "        iters.set_description(f\"{lr}\" + f\" {fun_log[-1]}\" + f\" {grad_log[-1]:.2f}\")\n",
    "        if fun_log[-1] < new_err:\n",
    "            break\n",
    "        else:\n",
    "            fun_log.append(new_err)\n",
    "    clear_output()\n",
    "    plt.plot(fun_log, label=\"obj\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(grad_log, label=\"grad_norm\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return X\n",
    "\n",
    "\n",
    "def compute_exp(\n",
    "    ground_truth_tt: List[torch.Tensor],\n",
    "    max_ranks: List[Tuple],\n",
    "    logger: Logger,\n",
    "    tuner: Optional[Callable] = None,\n",
    "    step: int = 2,\n",
    "    name=\"Hilbert\",\n",
    ") -> Tuple[dict, dict]:\n",
    "    def rel_err(target, X):\n",
    "        return ((target - X).norm() / target.norm()).cpu()\n",
    "\n",
    "    logger.info(\"=========EXP_STARTED===========\")\n",
    "    errs_to_ds = {}\n",
    "    params_am_to_ds = {}\n",
    "    N = len(ground_truth_tt)\n",
    "    shape = np.asarray([core.shape[1] for core in ground_truth_tt])\n",
    "    for _d_s in tqdm(range(1, N + 1, step)):\n",
    "        _d_t = N - _d_s\n",
    "        tucker_factors = [torch.eye(n=i, device=device) for i in shape[:_d_t]]\n",
    "        if _d_s:\n",
    "            tucker_factors.append(torch.eye(shape[_d_t], device=device))\n",
    "        ground_truth_sfett = TTSFTuckerTensor(\n",
    "            shared_factors_amount=_d_s,\n",
    "            tt_cores=ground_truth_tt,\n",
    "            tucker_factors=tucker_factors,\n",
    "        )\n",
    "        errs_to_ds[_d_s] = []\n",
    "        params_am_to_ds[_d_s] = []\n",
    "        wandb.init(\n",
    "            project=...,\n",
    "            name=name + f\"_tune={tuner is None}_ds={_d_s}\",\n",
    "            config={\"shared_factors\": _d_s, \"tuner\": (tuner is not None), \"name\": name},\n",
    "        )\n",
    "        log = {}\n",
    "        for max_r_t, max_r_tt in max_ranks:\n",
    "            ground_truth_sfett_rounded = ground_truth_sfett.clone()\n",
    "            ground_truth_sfett_rounded.round(sf_tucker_ranks=max_r_t, tt_ranks=max_r_tt)\n",
    "\n",
    "            logger.info(f\"====={max_r_t, max_r_tt,_d_s}=====\")\n",
    "            logger.info(f\"tt:{ground_truth_sfett_rounded.tt_ranks}\")\n",
    "            logger.info(f\"t:{ground_truth_sfett_rounded.tucker_ranks}\")\n",
    "            if tuner is not None:\n",
    "                logger.info(\n",
    "                    f\"before_tune:{rel_err(ground_truth_sfett, ground_truth_sfett_rounded)}\"\n",
    "                )\n",
    "\n",
    "                ground_truth_sfett_rounded = tuner(\n",
    "                    target=ground_truth_sfett, X=ground_truth_sfett_rounded\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"after_tune:{rel_err(ground_truth_sfett, ground_truth_sfett_rounded)}\"\n",
    "                )\n",
    "\n",
    "            errs_to_ds[_d_s].append(\n",
    "                rel_err(ground_truth_sfett, ground_truth_sfett_rounded)\n",
    "            )\n",
    "            log[\"params_am\"] = ground_truth_sfett_rounded.params_amount\n",
    "            log[\"rel_err\"] = errs_to_ds[_d_s][-1].item()\n",
    "            wandb.log(log)\n",
    "\n",
    "            params_am_to_ds[_d_s].append(ground_truth_sfett_rounded.params_amount)\n",
    "\n",
    "    wandb.finish()\n",
    "    return errs_to_ds, params_am_to_ds\n",
    "\n",
    "\n",
    "def Hilbert_fun(x, y, z, t, w, a, b, c, x1, x2, x3, x4):  # Input arguments are vectors\n",
    "    return 1 / (\n",
    "        1\n",
    "        + 2 * x\n",
    "        + 3 * y\n",
    "        + 4 * z\n",
    "        + 5 * t\n",
    "        + 6 * w\n",
    "        + 7 * a\n",
    "        + 8 * b\n",
    "        + 9 * c\n",
    "        + 10 * x1\n",
    "        + 11 * x2\n",
    "        + 12 * x3\n",
    "        + 13 * x4\n",
    "    )  # Hilbert tensor\n",
    "\n",
    "\n",
    "def create_exponent(n=32, alpha=-1):\n",
    "    def my_exp(*args):\n",
    "        res = args[0] / n\n",
    "        for i in range(1, len(args)):\n",
    "            res += args[i] / (n ** (i + 1))\n",
    "        return torch.exp(alpha * res**2)\n",
    "\n",
    "    return my_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 10\n",
    "n = 32\n",
    "alpha = -0.1\n",
    "domain = [torch.range(0, n - 1, device=device)] * l\n",
    "my_exp_tn = tn.cross(\n",
    "    function=create_exponent(alpha=alpha, n=n),\n",
    "    domain=domain,\n",
    "    device=device,\n",
    "    rmax=4,\n",
    ")\n",
    "my_exp_tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 12\n",
    "domain = [torch.linspace(0, 1, 512, device=device)] * l\n",
    "hilbert_like_tensor = tn.cross(\n",
    "    function=Hilbert_fun, domain=domain, device=device, eps=1e-14\n",
    ")\n",
    "hilbert_like_tensor, hilbert_like_tensor.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 16\n",
    "min_rank = 1\n",
    "max_ranks = [(i, j) for i in range(min_rank, max_rank, 1) for j in range(i, i + 2)]\n",
    "print(max_ranks, len(max_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_to_ds, params_am_to_ds = compute_exp(\n",
    "    ground_truth_tt=hilbert_like_tensor.clone().cores,\n",
    "    max_ranks=max_ranks,\n",
    "    logger=logger,\n",
    "    tuner=None,\n",
    "    step=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = ...\n",
    "# Extract data for a specific run\n",
    "\n",
    "# Convert to Pandas DataFrame for easy manipulation\n",
    "fig, ax = plt.subplots()\n",
    "for run in runs:\n",
    "    config = json.loads(run.json_config)\n",
    "    if \"name\" not in config or config[\"name\"][\"value\"] != \"Hilbert\":\n",
    "        continue\n",
    "    _d_s = config[\"shared_factors\"][\"value\"]\n",
    "    hist = pd.DataFrame(run.history(pandas=True))\n",
    "    ax.plot(\n",
    "        hist[\"params_am\"],\n",
    "        hist[\"rel_err\"],\n",
    "        label=f\"{('no' if _d_s == 1 or _d_s == 0 else _d_s)} shared factors\",\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"relative l2 error\")\n",
    "ax.set_xlabel(\"params amount\")\n",
    "ax.legend()\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for _d_s, errs in errs_to_ds.items():\n",
    "    ax.plot(\n",
    "        params_am_to_ds[_d_s],\n",
    "        errs,\n",
    "        label=f\"{('no' if _d_s == 1 or _d_s == 0 else _d_s)} shared factors\",\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"relative l2 error\")\n",
    "ax.set_xlabel(\"params amount\")\n",
    "ax.legend()\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_errs_to_ds, exp_params_am_to_ds = compute_exp(\n",
    "    ground_truth_tt=my_exp_tn.clone().cores,\n",
    "    max_ranks=max_ranks,\n",
    "    logger=logger,\n",
    "    tuner=None,\n",
    "    name=\"exp\",\n",
    "    step=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = ...\n",
    "# Extract data for a specific run\n",
    "\n",
    "# Convert to Pandas DataFrame for easy manipulation\n",
    "fig, ax = plt.subplots()\n",
    "for run in runs:\n",
    "    config = json.loads(run.json_config)\n",
    "    if \"name\" not in config or config[\"name\"][\"value\"] != \"exp\":\n",
    "        continue\n",
    "    _d_s = config[\"shared_factors\"][\"value\"]\n",
    "    hist = pd.DataFrame(run.history(pandas=True))\n",
    "    ax.plot(\n",
    "        hist[\"params_am\"],\n",
    "        hist[\"rel_err\"],\n",
    "        label=f\"{('no' if _d_s == 1 or _d_s == 0 else _d_s)} shared factors\",\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"relative l2 error\")\n",
    "ax.set_xlabel(\"params amount\")\n",
    "ax.legend()\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for _d_s, errs in exp_errs_to_ds.items():\n",
    "    ax.plot(\n",
    "        exp_params_am_to_ds[_d_s],\n",
    "        errs,\n",
    "        label=f\"{(1 if _d_s == 0 else _d_s)}-shared factors\",\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"relative l2 error\")\n",
    "ax.set_xlabel(\"params amount\")\n",
    "# ax.set_title(\n",
    "#     r\"Low rank approximation of $\\exp(-\\alpha x^2)$ tensor,\"\n",
    "#     + r\"$x \\in [0, 1]$\"\n",
    "#     + f\" {alpha = }\"\n",
    "# )\n",
    "ax.legend()\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
